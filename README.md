# 项目介绍
一个依赖于语言模型的英文拼写错误检测与自动纠正程序
是当时自然语言处理这门课的作业，属于是比较Old school的方法了（），效果十分依赖于从语料库提取出的n-gram语言模型和混淆矩阵。说是n-gram，但是受限于内存与运行速率限制，实际使用的也只有3-gram而已......
# 语言模型介绍
就这个项目而言，使用的是reuters 21578语料库（主要更大的语料库实在是用起来麻烦又难找），采用 modified Kneser-Ney 打折法插值平滑得到语言模型。
# 混淆矩阵介绍
使用了这个网址的1编辑距离错误数据来生成混淆矩阵（不用更高编辑距离的理由还是内存和速率问题......不过1编辑距离正常应该也是能涵盖大部分拼写错误的）
https://norvig.com/ngrams/
# 原理
不考虑真词错误的话，检错相对简单，只要对比是否有非词的错误并依据混淆矩阵纠正为正确词即可；考虑真词错误则要依赖语言模型和混淆矩阵通过贝叶斯公式得到搜索树，选择对应位置最大概率的词。贝叶斯公式计算概率导致语言模型会直接影响先验概率，所以效果极度依赖语言模型......也许用更多gram的语言模型和更高编辑距离的混淆矩阵能得到更好的效果，不过提高二者任一都会几何级提高计算难度（尤其是增加编辑距离会产生数倍于低编辑距离的候选词），所以最后并没有用更复杂的语言模型和混淆矩阵。

下面是当时报告里写的正经一点的原理介绍

本程序首先对文本中每个词进行遍历，生成与每个词编辑距离为2以内的字符串，并排除不在词汇表中的非词字符串，即得到编辑距离为2以内的候选词列表。
采用嘈杂信道模型对各候选词概率进行估计，由贝叶斯公式得
P(w│x)=(P(x│w)P(w))/(P(x))
又P(x)对于每个x而言可视作常数，即
P(w│x)~P(x│w)P(w)
故有
w ̂= argmax (P(x│w)P(w))
其中P(x│w)可由嘈杂信道模型的混淆矩阵由下式得到
P(deletion of y after x)=del[x,y]/count(xy) 
P(insertion of y after x)=ins[x,y]/count(x) 
P(substitution of x by y)=sub[x,y]/count(x) 
P(transposition of xy)=trans[x,y]/count(xy) 
	对于正确的词本身（即编辑距离为0），可设置一个总错误率ε，则其P(x│w)=1-ε
对于P(w)，可由语言模型得到其概率
将二者相乘即得到一个搜索树，选择值最大的路径即为纠正词
